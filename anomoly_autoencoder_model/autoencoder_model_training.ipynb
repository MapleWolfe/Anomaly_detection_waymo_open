{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MapleWolfe/Anomaly_detection_waymo_open/blob/main/anomoly_autoencoder_model/autoencoder_model_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "25GdaRiEvePn"
      },
      "source": [
        "# Building Model for Waymo open dataset\n",
        "- please see the auto_encoder_readme.md"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Enviornment Notebook was built in:\n",
        "- Kindly run this notebook in Google Colab\n",
        "\n",
        "##### Hardware needs:\n",
        "\n",
        "- Kindly use a minimum of 12 GB CPU ram\n",
        "- the standard Nvidia T4 GPU that comes with the free version of Google Colab.\n",
        "- 50 GB disk Minimum (less should work but this was the minimum during notebook creation)\n",
        "\n",
        "##### following files should be in local directory if using the local notebook\n",
        "- these files should be in the same folder as notebook\n",
        "\n",
        "##### instructions to run notebook,\n",
        "- Kindly fill in the code cell below to use the local parquet files stored in the same folder as notebook or the parquet files stored in a GCS bucket"
      ],
      "metadata": {
        "id": "d-VVlaMJ_S4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# do you want to use local sample files or google cloud storage?\n",
        "file_store = 'GCS' # kindly use either 'GCS' for Google cloud storage or 'LOCAL' for files stored in the same folder as notebook\n",
        "\n",
        "# leave both as None if using LOCAL or kindly replace this with your GCS api key json file and Bucket name or else\n",
        "gcs_json_key = '/content/organic-reef-390716-609989a4c6da.json' # please remember to start file path here with '/content/' + your json key file name\n",
        "bucket_name = 'waymo_sample_bucket'\n",
        "\n",
        "#these are the file download limits, it limits the number of parquet files being downloaded for training / validation / test\n",
        "train_limit = 2 #-1 means all relevant files in your google bucket\n",
        "val_limit = 1 #-1 means all relevant files in your google bucket\n",
        "test_limit = 1 #-1 means all relevant files in your google bucket\n",
        "\n",
        "\n",
        "# the path below is to the sample files: there are 4 parquets in total, you just need to upload them into colab enviornment\n",
        "train_box_data_file_path  = 'training_camera_box_10017090168044687777_6380_000_6400_000.parquet'\n",
        "train_image_data_file_path = 'training_camera_image_10017090168044687777_6380_000_6400_000.parquet'\n",
        "val_box_data_file_path = 'training_camera_box_10017090168044687777_6380_000_6400_000.parquet'\n",
        "val_image_data_file_path = 'training_camera_image_10017090168044687777_6380_000_6400_000.parquet'\n"
      ],
      "metadata": {
        "id": "TLmtrOPvfAgx"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1efbiYyBwl5V"
      },
      "source": [
        "## installs, imports, pre-sets\n",
        "\n",
        "- kindly open and run cell blocks based on the enviornment being run in to save computational resources."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Using detect_model_requirements.txt\n",
        "- please uncomment to use"
      ],
      "metadata": {
        "id": "4TZffgE6--bq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# uncomment to create notebook package requirments file called detect_model_requirements.txt\n",
        "#!pip freeze > detect_model_requirements.txt\n",
        "\n",
        "# use the code below to use detect_model_requirements.txt to install all necessary packages\n",
        "#!pip install -r detect_model_requirements.txt"
      ],
      "metadata": {
        "id": "AEw4lY1k8_EE"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neccessary installs on top of google colab\n",
        "- uncomment and Run this cell if you aren't using detect_model_requirements.txt and are operating in the GPU google colab enviornment"
      ],
      "metadata": {
        "id": "EXuACyZEEya7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "id": "pvf-dj0GvdxW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "141a08ec-110d-49e8-9c37-e851ea36ad4a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 52
        }
      ],
      "source": [
        "#!pip install google-cloud-storage\n",
        "#!pip install altair"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Imports"
      ],
      "metadata": {
        "id": "BuxoPuoiHKka"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "HrS7BbTI8vEQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11885d7c-f47a-4916-cf31-85aed963ca48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU_count: 1\n"
          ]
        }
      ],
      "source": [
        "# installs for google cloud storage\n",
        "from google.cloud import storage\n",
        "\n",
        "# general tool installs\n",
        "import os, io, shutil, warnings\n",
        "from tqdm.notebook import tqdm\n",
        "\n",
        "#image processing and plotting libraries\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as patches\n",
        "\n",
        "# data processing librarires\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# model evaluation\n",
        "from sklearn.model_selection import ParameterGrid\n",
        "\n",
        "# model libraries\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, UpSampling2D\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "#pre-sets\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "physical_devices = tf.config.list_physical_devices('GPU')\n",
        "print('GPU_count:' ,len(physical_devices))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## utility functions\n",
        "- functions that may be used at a later point"
      ],
      "metadata": {
        "id": "qE6iWVWNq0md"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### delete file function"
      ],
      "metadata": {
        "id": "GO9cCRwqq9G1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# code to delete a file\n",
        "def delete_file(file_path_list):\n",
        "  for a_file in (file_path_list):\n",
        "    os.remove(a_file)\n",
        "  return None"
      ],
      "metadata": {
        "id": "neffr318q_Fu"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### move copy function"
      ],
      "metadata": {
        "id": "KQOjtFvRR3rs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def move_copy(old_location, new_location):\n",
        "  shutil.copy(old_location, new_location)\n",
        "  return None"
      ],
      "metadata": {
        "id": "-wXmxJz8R87z"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### download blob function"
      ],
      "metadata": {
        "id": "QGybKhahuvLH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# to download a blob file\n",
        "def download_blob(a_blob,file_name):\n",
        "  a_blob.download_to_filename(file_name)\n",
        "  return None"
      ],
      "metadata": {
        "id": "0EXbcuJcuzq-"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Let's create folder directory for Autoencoder"
      ],
      "metadata": {
        "id": "JwM9qp-8tTl0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#folder name list for directory\n",
        "def make_directory(folder_name_list):\n",
        "  folder_path = os.path.join(*folder_name_list)\n",
        "  if not os.path.exists(folder_path):\n",
        "    os.makedirs(folder_path)\n",
        "\n",
        "  return None\n",
        "\n",
        "#function to build directory structure\n",
        "def build_a_directory():\n",
        "  for folder_type in ['train','test','eval']:\n",
        "    for data_type in ['images']:\n",
        "      make_directory(['datasets',folder_type,data_type])\n",
        "  return None\n",
        "\n"
      ],
      "metadata": {
        "id": "wPeKxiYvtMXt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# running directory function\n",
        "build_a_directory()"
      ],
      "metadata": {
        "id": "xDggpi2_voCu"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TnpY5qVHuE9x"
      },
      "source": [
        "## Google Cloud Storage section\n",
        "\n",
        "- Run these cells if you are using your private google cloud storage.\n",
        "- kindly ensure that your bucket has the same structure and file names as the waymo_open_dataset_v_2_0_0 bucket"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def start_gcs(gcs_json_key = gcs_json_key, bucket_name = bucket_name):\n",
        "# Please input API JSON KEY FOR your private google cloud storage where the files are kept in gcs_json_key defined in the first codeblock\n",
        "    os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = gcs_json_key\n",
        "    client = storage.Client()\n",
        "# replace the bucket name with the bucket name in your GCS, use the bucket_name object  in the first codeblock\n",
        "    bucket = client.get_bucket(bucket_name)\n",
        "\n",
        "#getting file list\n",
        "    files = bucket.list_blobs()\n",
        "    files_list = [a_file.name for a_file in files]\n",
        "    return bucket, files_list\n"
      ],
      "metadata": {
        "id": "PRZeQG2scdL_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GCS Iterative File Dowload function\n",
        "(Bare Image parquet and Bounding box parquet files)"
      ],
      "metadata": {
        "id": "EeMf3LBtorBu"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "Wi1hP1Hbs7sB"
      },
      "outputs": [],
      "source": [
        "# in the following function we are creating blobs: one blob object for images parquet and another blob for box coordinates parquet\n",
        "def open_gcs_file(files_list,bucket, storage_folder = 'training'):\n",
        "    # lets ensure that we have the correct file type\n",
        "    if storage_folder not in ['training','testing','validation']:\n",
        "      print('''please retype storage_folder it should either be ('training','testing','validation') in the second parameter of function)''')\n",
        "      return None\n",
        "\n",
        "    image_box_str = storage_folder +'/'+ 'camera_box'+'/'\n",
        "    bare_image_str = storage_folder +'/'+ 'camera_image'+'/'\n",
        "    print('files_list: ', files_list)\n",
        "    for a_file in files_list:\n",
        "        if image_box_str in a_file:\n",
        "          try:\n",
        "            box_file_name = a_file\n",
        "            box_file_blob = bucket.blob(box_file_name)\n",
        "            bare_image_file_name = box_file_name.replace(image_box_str, bare_image_str)\n",
        "            bare_image_blob = bucket.blob(bare_image_file_name)\n",
        "            if bare_image_blob is not None:\n",
        "              yield box_file_blob, os.path.basename(box_file_name),bare_image_blob, os.path.basename(bare_image_file_name)\n",
        "          except:\n",
        "            continue\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Storing files for autoencoder model\n",
        "- functions to unpack and store from parquet files"
      ],
      "metadata": {
        "id": "G4lVqS4YqYRW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### save path and create unique key function"
      ],
      "metadata": {
        "id": "hkq6ulNgZQpm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# creating a save path\n",
        "def create_path(file_id,store_type, images_labels = 'images'):\n",
        "  if store_type == 'train':\n",
        "    if images_labels == 'images':\n",
        "      return 'datasets/train/images/' +str(file_id) + '.jpg'\n",
        "    else:\n",
        "      return 'datasets/train/labels/' +str(file_id) + '.txt'\n",
        "\n",
        "  elif store_type == 'eval':\n",
        "    if images_labels == 'images':\n",
        "      return 'datasets/eval/images/' +str(file_id) + '.jpg'\n",
        "    else:\n",
        "      return 'datasets/eval/labels/' +str(file_id) + '.txt'\n",
        "  elif store_type == 'train':\n",
        "    if images_labels == 'images':\n",
        "      return 'datasets/train/images/' +str(file_id) + '.jpg'\n",
        "    else:\n",
        "      return 'datasets/train/labels/' +str(file_id) + '.txt'\n",
        "  else:\n",
        "    print('error type parameter: train / eval / test')\n",
        "\n",
        "# creating a key column\n",
        "def create_key_column(select_df):\n",
        "  return select_df['key.segment_context_name'].astype(str) +  select_df['key.frame_timestamp_micros'].astype(str) + select_df['key.camera_name'].astype(str)\n"
      ],
      "metadata": {
        "id": "GzLiKA0OY1ZF"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Building image pre-processing functions"
      ],
      "metadata": {
        "id": "57u-paccr8mA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this effectively converts all pixels within the bounding box into black pixels\n",
        "def blackout_bounding_boxes(df, image_array):\n",
        "    for index, row in df.iterrows():\n",
        "        center_x = int(row['center_x'])\n",
        "        center_y = int(row['center_y'])\n",
        "        width = int(row['size_x'])\n",
        "        height = int(row['size_y'])\n",
        "\n",
        "        x1 = int(center_x - width / 2)\n",
        "        y1 = int(center_y - height / 2)\n",
        "        x2 = int(center_x + width / 2)\n",
        "        y2 = int(center_y + height / 2)\n",
        "\n",
        "        image_array[y1:y2, x1:x2] = 0\n",
        "\n",
        "    return image_array\n",
        "\n",
        "# this will break the image into smaller images to be stacked\n",
        "\n",
        "def split_and_stack_image(image_array, output_shape=(64, 64)):\n",
        "    input_height, input_width = image_array.shape\n",
        "    num_rows = input_height // output_shape[0]\n",
        "    num_cols = input_width // output_shape[1]\n",
        "    stacked_images = []\n",
        "    for i in range(num_rows):\n",
        "        for j in range(num_cols):\n",
        "            start_row = i * output_shape[0]\n",
        "            end_row = start_row + output_shape[0]\n",
        "            start_col = j * output_shape[1]\n",
        "            end_col = start_col + output_shape[1]\n",
        "\n",
        "            small_image = image_array[start_row:end_row, start_col:end_col]\n",
        "            stacked_images.append(small_image)\n",
        "\n",
        "    stacked_array = np.stack(stacked_images, axis = 0)\n",
        "    return stacked_array\n"
      ],
      "metadata": {
        "id": "RFppQEyUr8-J"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Unpacking, scaling, storing images and annotations seperately"
      ],
      "metadata": {
        "id": "7_i3GODJY93L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# building bounding box labels\n",
        "# here we are also scaling the bounding box positions to the new image of 640 x 640\n",
        "def build_save_labels(df,image_size):\n",
        "  width, height = image_size\n",
        "  df.loc[:,'center_x'] = (df.loc[:,'[CameraBoxComponent].box.center.x'] * (640/width))\n",
        "  df.loc[:,'center_y'] = (df.loc[:,'[CameraBoxComponent].box.center.y'] * (640/height))\n",
        "  df.loc[:,'size_x'] = (df.loc[:,'[CameraBoxComponent].box.size.x'] * (640/width))\n",
        "  df.loc[:,'size_y'] = (df.loc[:,'[CameraBoxComponent].box.size.y'] * (640/height))\n",
        "\n",
        "  select_col_df = df[['[CameraBoxComponent].type','center_x','center_y','size_x','size_y']]\n",
        "  return select_col_df\n",
        "\n",
        "\n",
        "\n",
        "# constructing and saving image to path we are\n",
        "# here we are checking the image size and resizing it to 640 x 640\n",
        "# here we are also converting images to grayscale\n",
        "# here weare also\n",
        "def build_save_image(byte_string, box_df):\n",
        "  image_bytes = io.BytesIO(byte_string)\n",
        "  image = Image.open(image_bytes)\n",
        "  original_image_size = image.size\n",
        "\n",
        "  if (original_image_size[0] %32 == 0) and (original_image_size[1] %32 == 0):\n",
        "    #creating scaled bounding boxes\n",
        "    bounding_box_df = build_save_labels(box_df,original_image_size)\n",
        "\n",
        "    # resizing and greyscaling\n",
        "    resized_image = image.resize((640,640))\n",
        "    gray_img = np.mean(np.array(resized_image), axis=2, dtype=np.uint8)\n",
        "    #blackout areas within bounding boxes\n",
        "    blackout_img_array = blackout_bounding_boxes(bounding_box_df, gray_img)\n",
        "    split_img_array = split_and_stack_image(blackout_img_array, output_shape=(64, 64))\n",
        "    return split_img_array\n",
        "  else:\n",
        "    return 'fail'\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "g8iMbnDsY-UU"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# combining all functions above\n",
        "def unpack_store_images(image_parquet,box_parquet, store_type = 'train'):\n",
        "  file_path_store = []\n",
        "  image_df = pd.read_parquet(image_parquet)\n",
        "  box_df = pd.read_parquet(box_parquet)\n",
        "\n",
        "  image_df.loc[:,'key_column'] = create_key_column(image_df)\n",
        "  box_df.loc[:,'key_column'] = create_key_column(box_df)\n",
        "\n",
        "  commmon_key_list = list(set(box_df['key_column'].tolist()) & set(image_df['key_column'].tolist()))\n",
        "  id_iterator = 0\n",
        "  for common_key in tqdm(commmon_key_list):\n",
        "    key_loc_image = image_df.loc[image_df['key_column'] == common_key]\n",
        "    image_bytes = key_loc_image.reset_index().loc[0, '[CameraImageComponent].image']\n",
        "    key_loc_box = box_df.loc[box_df['key_column'] == common_key]\n",
        "\n",
        "    image_iterator = build_save_image(image_bytes,key_loc_box)\n",
        "    if image_iterator != 'fail':\n",
        "      for image_array in image_iterator:\n",
        "        image_path = create_path(id_iterator,store_type, images_labels = 'images')\n",
        "        id_iterator +=1\n",
        "        file_path_store.append(image_path)\n",
        "        output_image = Image.fromarray(image_array)\n",
        "        output_image.save(image_path)\n",
        "  return file_path_store"
      ],
      "metadata": {
        "id": "b7YA2v812Ghv"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## building model"
      ],
      "metadata": {
        "id": "bBH2p3xymiUk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AutoEncoder(tf.keras.Model):\n",
        "    def __init__(self, code_size=4, input_shape=(64, 64,1)):\n",
        "        super().__init__()\n",
        "        decoder_output_shape = input_shape[0] * input_shape[1]*1\n",
        "\n",
        "        self.encoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(32, activation='relu'),\n",
        "            tf.keras.layers.Dropout(0.1),\n",
        "            tf.keras.layers.Dense(16, activation='relu'),\n",
        "            tf.keras.layers.Dropout(0.1),\n",
        "            tf.keras.layers.Dense(8, activation='relu'),\n",
        "            tf.keras.layers.Dropout(0.1),\n",
        "            tf.keras.layers.Dense(code_size, activation='relu')\n",
        "        ])\n",
        "\n",
        "        self.decoder = tf.keras.Sequential([\n",
        "            tf.keras.layers.Dense(8, activation='relu'),\n",
        "            tf.keras.layers.Dropout(0.1),\n",
        "            tf.keras.layers.Dense(16, activation='relu'),\n",
        "            tf.keras.layers.Dropout(0.1),\n",
        "            tf.keras.layers.Dense(32, activation='relu'),\n",
        "            tf.keras.layers.Dropout(0.1),\n",
        "            tf.keras.layers.Dense(decoder_output_shape, activation='sigmoid')\n",
        "        ])\n",
        "\n",
        "    def call(self, inputs):\n",
        "        encoded = self.encoder(inputs)\n",
        "        decoded = self.decoder(encoded)\n",
        "        return decoded\n"
      ],
      "metadata": {
        "id": "difRp_-1L5rT"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def train_model(file_counter, model_path):\n",
        "  batch_size = 20\n",
        "  data_generator = ImageDataGenerator(rescale=1.0/255)\n",
        "  train_data = data_generator.flow_from_directory('/content/datasets/train/',batch_size= batch_size,class_mode='input', color_mode='grayscale', target_size=(64, 64))\n",
        "  validation_data = data_generator.flow_from_directory('/content/datasets/eval/',batch_size= batch_size,class_mode='input', color_mode='grayscale', target_size=(64, 64))\n",
        "  if file_counter == 0:\n",
        "    model = AutoEncoder(code_size=64)\n",
        "    model.compile(loss='msle', metrics=['mse'], optimizer='adam')\n",
        "\n",
        "    fitted_encoder_history = model.fit(train_data, steps_per_epoch=train_data.samples // batch_size,\n",
        "      epochs=1,validation_data=validation_data, validation_steps=validation_data.samples // batch_size)\n",
        "\n",
        "    model.save(model_path,save_format=\"tf\" )\n",
        "    return model_path\n",
        "\n",
        "  else:\n",
        "    model = tf.keras.models.load_model(model_path)\n",
        "    model.compile(loss='msle', metrics=['mse'], optimizer='adam')\n",
        "\n",
        "    fitted_encoder_history = model.fit(train_data, steps_per_epoch=train_data.samples // batch_size,\n",
        "      epochs=3,validation_data=validation_data, validation_steps=validation_data.samples // batch_size)\n",
        "\n",
        "\n",
        "    return model, model_path\n"
      ],
      "metadata": {
        "id": "5arGsX1wmqUJ"
      },
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating validation data"
      ],
      "metadata": {
        "id": "lHpjdrljGkTj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking and then initiating GCS file download\n",
        "if file_store == 'GCS':\n",
        "  bucket, files_list = start_gcs()\n",
        "  gcs_iterator = open_gcs_file(files_list,bucket, storage_folder = 'validation')\n",
        "  file_counter = 0\n",
        "  train_file_path_list = []\n",
        "  for a_file in gcs_iterator:\n",
        "    # evaluation data\n",
        "    box_file_blob, box_file_name,bare_image_blob, bare_image_file_name = a_file\n",
        "\n",
        "    download_blob(box_file_blob,box_file_name)\n",
        "    new_box_file_name = 'box_file_'+ box_file_name\n",
        "    os.rename(box_file_name, new_box_file_name)\n",
        "\n",
        "    download_blob(bare_image_blob,bare_image_file_name)\n",
        "    new_image_file_name = 'image_file_'+ bare_image_file_name\n",
        "    os.rename(bare_image_file_name, new_image_file_name)\n",
        "\n",
        "    eval_file_path_list = unpack_store_images(new_image_file_name,new_box_file_name, store_type = 'eval')\n",
        "    delete_file([new_box_file_name,new_image_file_name])\n",
        "\n",
        "    print('completed_file_number: ',file_counter)\n",
        "    file_counter += 1\n",
        "    if (file_counter >= val_limit) and (val_limit != -1):\n",
        "      break\n",
        "\n",
        "elif file_store == 'LOCAL':\n",
        "    train_file_path_list = unpack_store_images(val_box_data_file_path,val_image_data_file_path, store_type = 'eval')\n",
        "else:\n",
        "  print('''check file_store variable, it should be in capital 'GCS' or 'LOCAL' ''')\n"
      ],
      "metadata": {
        "id": "QFtHGq_aGn_U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## main run to create data store and train model"
      ],
      "metadata": {
        "id": "VphlxgrHyf4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking and then initiating GCS file download\n",
        "model_path = 'encoder_model'\n",
        "if file_store == 'GCS':\n",
        "  bucket, files_list = start_gcs()\n",
        "  gcs_iterator = open_gcs_file(files_list,bucket, storage_folder = 'training')\n",
        "  file_counter = 0\n",
        "  train_file_path_list = []\n",
        "  for a_file in gcs_iterator:\n",
        "    # train data\n",
        "      box_file_blob, box_file_name,bare_image_blob, bare_image_file_name = a_file\n",
        "\n",
        "      download_blob(box_file_blob,box_file_name)\n",
        "      new_box_file_name = 'box_file_'+ box_file_name\n",
        "      os.rename(box_file_name, new_box_file_name)\n",
        "\n",
        "      download_blob(bare_image_blob,bare_image_file_name)\n",
        "      new_image_file_name = 'image_file_'+ bare_image_file_name\n",
        "      os.rename(bare_image_file_name, new_image_file_name)\n",
        "\n",
        "      train_file_path_list = unpack_store_images(new_image_file_name,new_box_file_name, store_type = 'train')\n",
        "\n",
        "      model_path = train_model(file_counter, model_path)\n",
        "      print('completed_file_number: ',file_counter)\n",
        "      file_counter += 1\n",
        "\n",
        "      delete_file([new_box_file_name,new_image_file_name]+train_file_path_list)\n",
        "      if (file_counter >= train_limit) and (train_limit != -1):\n",
        "        break\n",
        "\n",
        "elif file_store == 'LOCAL':\n",
        "    file_counter = 0\n",
        "    model_list = []\n",
        "    train_file_path_list = unpack_store_images(train_box_data_file_path,train_image_data_file_path, store_type = 'train')\n",
        "\n",
        "    model_list = train_model(file_counter, model_path)\n",
        "    delete_file([train_box_data_file_path,train_image_data_file_path]+train_file_path_list)\n",
        "else:\n",
        "  print('''check file_store variable, it should be in capital 'GCS' or 'LOCAL' ''')\n"
      ],
      "metadata": {
        "id": "CePd2VYmgOxZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model evaluation and comparison"
      ],
      "metadata": {
        "id": "Ou4B6On87oTr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hElcLbaZ7z_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Bounding Box Building Functions"
      ],
      "metadata": {
        "id": "ebbcaF6GUiBK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#to plot one box\n",
        "def plot_a_box(box_spec_list):\n",
        "  print(box_spec_list)\n",
        "  center_x = box_spec_list[0]\n",
        "  center_y = box_spec_list[1]\n",
        "  width = box_spec_list[2]\n",
        "  height =box_spec_list[3]\n",
        "  x_min = center_x - (width / 2)\n",
        "  y_min = center_y - (height / 2)\n",
        "  rect = patches.Rectangle((x_min, y_min), width, height,linewidth=2, edgecolor='gold', facecolor='none')\n",
        "  return rect\n",
        "\n",
        "# to plot all boxes\n",
        "def plot_boxes(image_object,box_iterator_object):\n",
        "  fig, ax = plt.subplots()\n",
        "  ax.imshow(image_object)\n",
        "  for a_box in box_iterator_object:\n",
        "    one_box_val = a_box.xywh[0].tolist()\n",
        "    one_plot_box = plot_a_box(one_box_val)\n",
        "    ax.add_patch(one_plot_box)\n",
        "  ax.set_xlim(0, image_object.width)\n",
        "  return fig, ax"
      ],
      "metadata": {
        "id": "QiFK2CVEUhXn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "udx4BLaPIeLo"
      },
      "source": [
        "## Model building"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "REKDRI7jIgkM"
      },
      "outputs": [],
      "source": [
        "model = YOLO('yolov8n.pt')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pLj8j-IUJdyN",
        "outputId": "80cc2345-ce44-4d7a-c430-3abb4365a913"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'PIL.JpegImagePlugin.JpegImageFile'>\n"
          ]
        }
      ],
      "source": [
        "a_image = one_image_df.loc[0,'[CameraImageComponent].image']\n",
        "image_bytes = io.BytesIO(a_image)\n",
        "image = Image.open(image_bytes)\n",
        "print(type(image))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JDpGr0cdJ0Ra"
      },
      "outputs": [],
      "source": [
        "results = model(image)\n",
        "for result in results:\n",
        "  boxes = result.boxes  # Boxes object for bbox outputs\n",
        "  fig, ax = plot_boxes(image,boxes)\n",
        "  plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "boxes[0].xywh[0].tolist()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kQUM5f23PInC",
        "outputId": "4bba3200-377a-45a2-ff05-aebc6980e5a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1090.59765625, 543.5984497070312, 149.1917724609375, 142.18844604492188]"
            ]
          },
          "metadata": {},
          "execution_count": 25
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "4TZffgE6--bq",
        "EXuACyZEEya7",
        "qE6iWVWNq0md",
        "TnpY5qVHuE9x",
        "hkq6ulNgZQpm",
        "Ou4B6On87oTr",
        "ebbcaF6GUiBK",
        "udx4BLaPIeLo"
      ],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}